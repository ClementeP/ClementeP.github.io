---
---

@inproceedings{
loula2025syntactic,
title={Syntactic and Semantic Control of Large Language Models via Sequential Monte Carlo},
author={Jo{\~a}o Loula and Benjamin LeBrun and Li Du and Ben Lipkin and Clemente Pasti and Gabriel Grand and Tianyu Liu and Yahya Emara and Marjorie Freedman and Jason Eisner and Ryan Cotterell and Vikash Mansinghka and Alexander K. Lew and Tim Vieira and Timothy J. O'Donnell},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
selected={true},
url={https://openreview.net/forum?id=xoXn62FzD0},
abstract={A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as probabilistic conditioning, but exact generation from the resulting distribution—which can differ substantially from the LM’s base distribution—is generally intractable. In this work, we develop an architecture for controlled LM generation based on sequential Monte Carlo (SMC). Our SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computational resources in light of new information during the course of generation. By comparing to a number of alternatives and ablations on four challenging domains---Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis—we demonstrate that, with little overhead, our approach allows small open-source language models to outperform models over 8
 larger, as well as closed-source, fine-tuned ones. In support of the probabilistic perspective, we show that these performance improvements are driven by better approximation to the posterior distribution. Our system builds on the framework of Lew et al. (2023) and integrates with its language model probabilistic programming language, giving users a simple, programmable way to apply SMC to a broad variety of controlled generation problems.},
}

@inproceedings{pasti-etal-2023-intersection,
    title = "On the Intersection of Context-Free and Regular Languages",
    author = "Pasti, Clemente  and
      Opedal, Andreas  and
      Pimentel, Tiago  and
      Vieira, Tim  and
      Eisner, Jason  and
      Cotterell, Ryan",
    editor = "Vlachos, Andreas  and
      Augenstein, Isabelle",
    booktitle = "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.eacl-main.52/",
    doi = "10.18653/v1/2023.eacl-main.52",
    pages = "737--749",
    selected={true},
    abstract = "The Bar-Hillel construction is a classic result in formal language theory. It shows, by a simple construction, that the intersection of a context-free language and a regular language is itself context-free. In the construction, the regular language is specified by a finite-state automaton. However, neither the original construction (Bar-Hillel et al., 1961) nor its weighted extension (Nederhof and Satta, 2003) can handle finite-state automata with {\ensuremath{\varepsilon}}-arcs. While it is possible to remove {\ensuremath{\varepsilon}}-arcs from a finite-state automaton efficiently without modifying the language, such an operation modifies the automaton{'}s set of paths. We give a construction that generalizes the Bar- Hillel in the case the desired automaton has {\ensuremath{\varepsilon}}-arcs, and further prove that our generalized construction leads to a grammar that encodes the structure of both the input automaton and grammar while retaining the asymptotic size of the original construction."
}

@inproceedings{vieira2025language,
title={Language Models over Canonical Byte-Pair Encodings},
author={Tim Vieira and Tianyu Liu and Clemente Pasti and Yahya Emara and Brian DuSell and Benjamin LeBrun and Mario Giulianelli and Juan Luis Gastaldi and John Terilla and Timothy J. O'Donnell and Ryan Cotterell},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=eCVrfVDNSY},
abstract={Modern language models represent probability distributions over character strings as distributions over (shorter) token strings derived via a deterministic tokenizer, such as byte-pair encoding. While this approach is highly effective at scaling up language models to large corpora, its current incarnations have a concerning property: the model assigns nonzero probability mass to an exponential number of noncanonical token encodings of each character string -- these are token strings that decode to valid character strings but are impossible under the deterministic tokenizer (i.e., they will never be seen in any training corpus, no matter how large). This misallocation is both erroneous, as noncanonical strings never appear in training data, and wasteful, diverting probability mass away from plausible outputs. These are avoidable mistakes! In this work, we propose methods to enforce canonicality in token-level language models, ensuring that only canonical token strings are assigned positive probability. We present two approaches: (1) canonicality by conditioning, leveraging test-time inference strategies without additional training, and (2) canonicality by construction, a model parameterization that guarantees canonical outputs but requires training. We demonstrate that fixing canonicality mistakes improves the likelihood of held-out data for several models and corpora.},
}

@inproceedings{pasti-etal-2024-l,
    title = "An {L}* Algorithm for Deterministic Weighted Regular Languages",
    author = {Pasti, Clemente  and
      Karag{\"o}z, Talu  and
      Nowak, Franz  and
      Svete, Anej  and
      Boumasmoud, Reda  and
      Cotterell, Ryan},
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.468/",
    doi = "10.18653/v1/2024.emnlp-main.468",
    pages = "8197--8210",
    selected={true},
    abstract = "Extracting finite state automata (FSAs) fromblack-box models offers a powerful approachto gaining interpretable insights into complexmodel behaviors. To support this pursuit, wepresent a weighted variant of Angluin{'}s (1987)L* algorithm for learning FSAs. We stay faithful to the original formulation, devising a wayto exactly learn deterministic weighted FSAswhose weights support division. Furthermore,we formulate the learning process in a mannerthat highlights the connection with FSA minimization, showing how L* directly learns aminimal automaton for the target language."
}

